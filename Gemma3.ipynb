{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7r4kNuFQTbJ"
   },
   "source": [
    "#Build Gemma3 270M from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijDG3IDqkjKx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Let us build a Small Language Model (SLM) from scratch. We will try to keep the parameter size to 50-60 million.\n",
    "\n",
    "Our goal is to generate creative and coherent text based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSeXbOztRtKN",
    "outputId": "753a6a46-74ee-4588-d72f-eac9274698c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkpPWqR8-tFO",
    "outputId": "8aab0d98-87a6-4183-e7c3-dbff04c13709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install -U datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "referenced_widgets": [
      "76714c196e734bebbc968661c7cdd243",
      "16705faff76f41a2ab0df76ffa807451",
      "701ec2e728e04b5284a32480c7855364",
      "8f038ded480c451baf596125166ed7a4",
      "a7ac11a17612485e8ef090cfe4272c5d",
      "48e225e280014d4fab626ebfc8da3474",
      "fc23c33d3a544a53b70e87d504b6f6bd",
      "b7d1e880deda43f081616f848a54c282",
      "30e76f44e63d4f54bb8821986fc192c7",
      "63dc7de46a48475f9041638c16d9bcae",
      "40839d5860274ce782c65aa93e2f4e6b",
      "f6329d025a9445f1b668c117646e2ad7",
      "fc67139955e048d2b3fb96d08c872c93",
      "2791c5637990421cbf57fc92016bb3f4",
      "14be12435d614dbe8039692ddc04c0ae",
      "5ee10177a58e4554a3cd24d42636c8ce",
      "8cc83a488b214e7c93b3981b53f6ea26",
      "3549c2f408df4d3ab9647264ad7c1297",
      "7c908c3cef48431180b018a4d1416b5e",
      "286011d5850c48f0bddf45f523a580f5",
      "68359cc90c224524a57d08715ceea071",
      "41e174268fd049c8952c044809fc3bae",
      "705f2f2e35e745f398014b3082f4b5d6",
      "a63a42e4271040ad88176c334920fe01",
      "d09331d00a924529bca9cd4c3abd988f",
      "078998f383274646a955a016be82fcf2",
      "596c6a0788c2479d914fb733b5ec1db5",
      "8fc99a501e3e4eea9a2cd48cf065dfba",
      "b299bc5d58c04fd5beb18c47245bc88a",
      "471862a80b5e41778492979478b3ea55",
      "f776e8ea1c3b4b4494a05be651bf736b",
      "f5d6dd1247134d4596ce47cd21875bc6",
      "aa19a3345b9c4803bb35607fa65a13ac",
      "bf6aa327313148cca9bd29c41b17d39b",
      "4b17ff33557a45c6beb8379bb039ee22",
      "d5f1cd2de6b346c2bc2013074d2b77b7",
      "4ef0cff4ff68422f81dd28558c722d35",
      "d3b82afbc98c429298a194f989822d45",
      "917f794496f7422fbc715a5dc7cb042b",
      "f4e7d6732ed54cffa3dcedc9c02d128a",
      "ff27bcd4ff624c729760c3d7cc021fdf",
      "1e3d658d47a14a228ddaed66e60277d0",
      "2b7314ceb60b489da01096368ba61dc4",
      "1c789361e26a486bbe9794f0985d9fa7",
      "95df2fe5e19e483699aafaf5c6dfceb2",
      "ece03262e3fb429bae55cacc339f88ff",
      "5bbaa2c141fa41698fe28b0564bedb7d",
      "becdcc6e689649c4a5430eddb761f36c",
      "0dfeab8cea644125a48117f18eccf267",
      "eb9adcebefc24c60bbba8fbec0a871a0",
      "6ae84d3337634360a413a610a588e0ed",
      "34360b78c2b54797a174e4d7c4d7762b",
      "e0639ec994fd417fb226b90c5126c353",
      "d4b7cd73122c4fa9ad3782601728e41d",
      "72ee8c04aca44dacbcf36432a980d6a6",
      "ba721f351b004b8782c3582df125fd15",
      "3e7dfdfb8899440cbda4825d19817f56",
      "b0182709de014357884438659c28b66d",
      "2221a8dcdd2341d1815c49b9701c2cfe",
      "ee7b338ec6004f6eb5fac991cce6e9f2",
      "0eb7e9e5b3dd4a9eadbb4c03fea7e452",
      "fb917c17336340d789c191fc3df3f822",
      "efacc72dd3b84d858890c1d0f93cc5df",
      "7909a069a2f94c45984037b215ba4e76",
      "22408293761944fa8e5af45833df4be1",
      "35d9df329dc44d17ba3f9029beb56afe",
      "c5cec1476c62450a94c386f08c30228b",
      "8d87e33d7529486192e9a13d35568807",
      "e8e421d9446440228a909d14e1ee136e",
      "c48c1afe4974480a93fb55d6e9781e02",
      "2731757277ed4838bcc0d5de1403bc8d",
      "8e817532b4c34fa7a5946ddb9f5f16d7",
      "0b17c175a1ca421c9cfdb5ebe4f57663",
      "54ca15a8746f466b806cc943f7bf7463",
      "c682eb2c95c1458db09d9cfb9801772d",
      "c27018a98d3d42b1bb7d3cbeed305631",
      "0bb05305b60e4ec0958565bb2c70f4f4",
      "5cec26ac71f4420482a937d7ec300445",
      "1e5e29c151554735a99d396590311a88",
      "15b2f0b9761442958cde7db7642cf590",
      "cf2e8437ea74438eba5f4047352c4865",
      "8955a753870241fda507c590cbafce90",
      "0db57d097b004f73a0fb31d19cb4005c",
      "a333d24e43914b30a67a5631797160e7",
      "ae704221bbe046f5bd2063223bc9008d",
      "5fc540f64a15422f8b529052e716db0d",
      "536a813ccbd9488aa42ec3c73188f2bf",
      "64a3b26bf03045d9b60a5386c096e8f2"
     ]
    },
    "id": "3r34TTsnR3GI",
    "outputId": "96cc136b-46a6-4daf-a4d0-4c7f744e367d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76714c196e734bebbc968661c7cdd243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6329d025a9445f1b668c117646e2ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705f2f2e35e745f398014b3082f4b5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6aa327313148cca9bd29c41b17d39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95df2fe5e19e483699aafaf5c6dfceb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba721f351b004b8782c3582df125fd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cec1476c62450a94c386f08c30228b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cec26ac71f4420482a937d7ec300445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vccyr4qKR6OH"
   },
   "source": [
    "## Step 2: Tokenize the Dataset\n",
    "\n",
    "In this step, we will do the following:\n",
    "\n",
    "(1) Tokenize the dataset into tokenIDs.\n",
    "\n",
    "(2) Create a file called \"train.bin\" and \"validtion.bin\" where we will store the tokenIDs from the entire dataset.\n",
    "\n",
    "(3) We make sure the tokenIDs are stored on a disk, rather than on the RAM for efficient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "ff84e9b3b3ea496ca7f131709ed3fa27",
      "a4e7e1b497ac4d2280f56cd00cdca20c",
      "195f1761d0e346de849001d782088410",
      "e0919078c145400eb75602549f8862f5",
      "541d115ea9af43cd991987456f1d92b7",
      "db01709ee22b4addb2404e36b4456cd6",
      "6975846054be407fa62e9aa64c5da200",
      "615a669a797c4412945351d5fa690d10",
      "8209b7d8d09e45c3826f624f157d9546",
      "7367903d6ac14a8bb1c4e814fa1896f4",
      "f74d2c891a03419aa5f6860189711dd7",
      "0a105957535343b192a3c16d20b719e3",
      "f916a95890ba4b49ada2ad1085ffd5a5",
      "b444ca0161624d48a68697f4f5001017",
      "13c62630405f4414b1d0fc68e9ea1c40",
      "014388d80e664c79bc88cac928b39352",
      "bec65bb811be406e9dcb543a555d2b15",
      "831f4605d64341b1aadd3a00aa9fc4bf",
      "194dd73a507d43c29f7a70b8a76a76aa",
      "3ac5f377b7524019a12756e3fe4c253e",
      "f78dd3056f004dfbac29d94fced47687",
      "6c244a40012d4037b0dff5bac2e46cb6",
      "d1078e7705f74b6787a658614de357a4",
      "9974891863f34e3ca59ef7384378ed71",
      "a918c48dc2f64c0cb67e6ada08bced9a",
      "e181fd7fb6f844c8b89ba05157c6c3af",
      "7513b7f9cf394a4e9cd2e77910e7e1a6",
      "56ab21269d544b4080cec10aa4055de2",
      "8365807a87e9477fbac47e4ed68c349d",
      "c19c692b320e468197ffa43a065a71d0",
      "45bf39234e144d2ea7a1c48e02032aaf",
      "32dcbfc13e2a4854ad42c0196a5f7a60",
      "508ea1c2a5b74e4890fbc918c95fcbe5",
      "712c964fe6184b8d94c8023fc14f4d4f",
      "efcc9abab8354c80b9b0d0a43ce477b7",
      "f06af4c0ce7c4e11bd2dda149ac431ff",
      "46157fc594da49bebfa3b76a0f7a9286",
      "fc055e4ff00d4fe18f1957e7e0141577",
      "73d6ae81829a4fc08cdf52a8d7c1a794",
      "15cec2c543984995a6c4cc041f087ff4",
      "eb4c28871c0d4f7ba295de81256c74b0",
      "b6c368a57b8942318ba1b61092373c6e",
      "e884b9abb5de427dae87de21c53ef656",
      "b4194e98bba7439b8664ddbd5fed90d1"
     ]
    },
    "id": "vFkgAjyMR8fa",
    "outputId": "51c2a371-518e-4512-b6e3-5e64f2d0510e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff84e9b3b3ea496ca7f131709ed3fa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing the splits (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a105957535343b192a3c16d20b719e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing the splits (num_proc=8):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1078e7705f74b6787a658614de357a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing train.bin:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712c964fe6184b8d94c8023fc14f4d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing validation.bin:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "        )\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_qRtn_WSbV4"
   },
   "source": [
    "## Step 3: Create Input-Output batches for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gak79CZESkjN"
   },
   "outputs": [],
   "source": [
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
    "#block size = context window\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA1SVp1hkjKy"
   },
   "source": [
    "## Step 4: Define the SLM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsjXJXs2FVJu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9friaxWABOA-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # Gemma3 stores zero-centered weights and uses (1 + weight) during forward\n",
    "        self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Match HF Gemma3: compute norm in float32, then scale by (1 + w)\n",
    "        input_dtype = x.dtype\n",
    "        x_f = x.float()\n",
    "        var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x_f * torch.rsqrt(var + self.eps)\n",
    "        out = x_norm * (1.0 + self.scale.float())\n",
    "\n",
    "        if self.shift is not None:\n",
    "            out = out + self.shift.float()\n",
    "\n",
    "        return out.to(input_dtype)\n",
    "\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False,\n",
    "        query_pre_attn_scalar=None, dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "        if query_pre_attn_scalar is not None:\n",
    "            self.scaling = (query_pre_attn_scalar) ** -0.5\n",
    "        else:\n",
    "            self.scaling = (head_dim) ** -0.5\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Scale queries\n",
    "        queries = queries * self.scaling\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: dict, attn_type: str):\n",
    "        super().__init__()\n",
    "        self.attn_type = attn_type\n",
    "\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
    "            dtype=cfg[\"dtype\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask_global,\n",
    "        mask_local,\n",
    "        cos_global,\n",
    "        sin_global,\n",
    "        cos_local,\n",
    "        sin_local,\n",
    "    ):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        if self.attn_type == \"sliding_attention\":\n",
    "            attn_mask = mask_local\n",
    "            cos = cos_local\n",
    "            sin = sin_local\n",
    "        else:\n",
    "            attn_mask = mask_global\n",
    "            cos = cos_global\n",
    "            sin = sin_global\n",
    "\n",
    "        x_attn = self.att(x, attn_mask, cos, sin)\n",
    "        x_attn = self.post_attention_layernorm(x_attn)\n",
    "        x = shortcut + x_attn\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x_ffn = self.pre_feedforward_layernorm(x)\n",
    "        x_ffn = self.ff(x_ffn)\n",
    "        x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
    "        x = shortcut + x_ffn\n",
    "        return x\n",
    "\n",
    "class Gemma3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
    "        ])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Reusuable utilities\n",
    "        cos_local, sin_local = compute_rope_params(\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            theta_base=cfg[\"rope_local_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        cos_global, sin_global = compute_rope_params(\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
    "        self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
    "        self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
    "        self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
    "\n",
    "    def _create_masks(self, seq_len, device):\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "\n",
    "        # mask_global (future is masked: j > i)\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        #  i\n",
    "        #     0:  0 1 1 1 1 1 1 1\n",
    "        #     1:  0 0 1 1 1 1 1 1\n",
    "        #     2:  0 0 0 1 1 1 1 1\n",
    "        #     3:  0 0 0 0 1 1 1 1\n",
    "        #     4:  0 0 0 0 0 1 1 1\n",
    "        #     5:  0 0 0 0 0 0 1 1\n",
    "        #     6:  0 0 0 0 0 0 0 1\n",
    "        #     7:  0 0 0 0 0 0 0 0\n",
    "        mask_global = torch.triu(ones, diagonal=1)\n",
    "\n",
    "        # far_past (too far back is masked: i - j >= sliding_window)\n",
    "        # where sliding_window = 4\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        #  i\n",
    "        #     0:  0 0 0 0 0 0 0 0\n",
    "        #     1:  0 0 0 0 0 0 0 0\n",
    "        #     2:  0 0 0 0 0 0 0 0\n",
    "        #     3:  0 0 0 0 0 0 0 0\n",
    "        #     4:  1 0 0 0 0 0 0 0\n",
    "        #     5:  1 1 0 0 0 0 0 0\n",
    "        #     6:  1 1 1 0 0 0 0 0\n",
    "        #     7:  1 1 1 1 0 0 0 0\n",
    "        far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
    "\n",
    "        # Local (sliding_window) = future OR far-past\n",
    "        # mask_local\n",
    "        #     j:  0 1 2 3 4 5 6 7\n",
    "        # i\n",
    "        # 0:      0 1 1 1 1 1 1 1\n",
    "        # 1:      0 0 1 1 1 1 1 1\n",
    "        # 2:      0 0 0 1 1 1 1 1\n",
    "        # 3:      0 0 0 0 1 1 1 1\n",
    "        # 4:      1 0 0 0 0 1 1 1\n",
    "        # 5:      1 1 0 0 0 0 1 1\n",
    "        # 6:      1 1 1 0 0 0 0 1\n",
    "        # 7:      1 1 1 1 0 0 0 0\n",
    "        mask_local = mask_global | far_past\n",
    "        return mask_global, mask_local\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        b, seq_len = input_ids.shape\n",
    "        x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
    "        mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(\n",
    "                x,\n",
    "                mask_global=mask_global,\n",
    "                mask_local=mask_local,\n",
    "                cos_global=self.cos_global,\n",
    "                sin_global=self.sin_global,\n",
    "                cos_local=self.cos_local,\n",
    "                sin_local=self.sin_local,\n",
    "            )\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "      for _ in range(max_new_tokens):\n",
    "        ctx_len = self.cfg[\"context_length\"]\n",
    "        idx_cond = idx if idx.size(1) <= ctx_len else idx[:, -ctx_len:]\n",
    "        logits, _ = self(idx_cond)  # targets=None by default\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "      return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRm6WlvfkjKz"
   },
   "outputs": [],
   "source": [
    "GEMMA3_CONFIG_270M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 32_768,\n",
    "    \"emb_dim\": 640,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 18,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"head_dim\": 256,\n",
    "    \"qk_norm\": True,\n",
    "    \"n_kv_groups\": 1,\n",
    "    \"rope_local_base\": 10_000.0,\n",
    "    \"rope_base\": 1_000_000.0,\n",
    "    \"sliding_window\": 512,\n",
    "      \"layer_types\": [\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\"\n",
    "    ],\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"query_pre_attn_scalar\": 256,\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_a8Rd-0S_WC"
   },
   "source": [
    "## Step 5: Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La2Aun_nTBzk"
   },
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvqWPUstTRXO"
   },
   "source": [
    "## Step 6: Define SLM Training Configuration Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QQyayhnkjK5",
    "outputId": "cbbf19f4-d13c-429b-b478-2032ccaad9a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c1949366d50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_iters = 150000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "\n",
    "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
    "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmWj6YcKTW_z"
   },
   "source": [
    "## Step 7: Define SLM Training Configuration Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMkO3FlNkjK6",
    "outputId": "ff070df6-bced-45ea-9b59-2e8112ce69c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2043281014.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz8fPSKNTY3W"
   },
   "source": [
    "## Step 8: Pre-train the SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "de50d4a2bbab4abeb54abcdf23f9dd68",
      "4585797cd6454498a4af437e02411dde",
      "be01ca1fe4cf49e79c562625b13e19ed",
      "f243057c3b4f4804a4dccac9103627aa",
      "d1b19ae0e8714cac8c6769cb80d0e98f",
      "69590be94af2401f9c04530773fda71f",
      "3079ecdfe24f4e498b9a3d7dd0f077e2",
      "3eb66bcb73134594ae43b07d4563de9f",
      "cadde4f8f1324a86919c0ff5bb55b2b6",
      "55184cbb98a247d0bf5c658866b6344c",
      "e947ac5edbb64e52913a9cbd60a2667d"
     ]
    },
    "id": "t0l-YhockjK6",
    "outputId": "5b606a5a-9549-4ca1-e1e9-20dd8d8fdbd2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de50d4a2bbab4abeb54abcdf23f9dd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 9.7023, val loss 9.7089\n",
      "The current learning rate: 0.00007\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 7.9667, val loss 7.9696\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1500: train loss 6.8167, val loss 6.8163\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2000: train loss 5.8444, val loss 5.8524\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2500: train loss 5.1325, val loss 5.1262\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3000: train loss 4.6965, val loss 4.6994\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3500: train loss 4.3937, val loss 4.3979\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4000: train loss 4.1975, val loss 4.1969\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4500: train loss 4.0489, val loss 4.0432\n",
      "The current learning rate: 0.00010\n",
      "Epoch 5000: train loss 3.9225, val loss 3.9234\n",
      "The current learning rate: 0.00010\n",
      "Epoch 5500: train loss 3.8216, val loss 3.8208\n",
      "The current learning rate: 0.00010\n",
      "Epoch 6000: train loss 3.7346, val loss 3.7394\n",
      "The current learning rate: 0.00010\n",
      "Epoch 6500: train loss 3.6620, val loss 3.6568\n",
      "The current learning rate: 0.00010\n",
      "Epoch 7000: train loss 3.6035, val loss 3.5961\n",
      "The current learning rate: 0.00010\n",
      "Epoch 7500: train loss 3.5424, val loss 3.5392\n",
      "The current learning rate: 0.00010\n",
      "Epoch 8000: train loss 3.4941, val loss 3.5015\n",
      "The current learning rate: 0.00010\n",
      "Epoch 8500: train loss 3.4448, val loss 3.4580\n",
      "The current learning rate: 0.00010\n",
      "Epoch 9000: train loss 3.4036, val loss 3.4075\n",
      "The current learning rate: 0.00010\n",
      "Epoch 9500: train loss 3.3760, val loss 3.3743\n",
      "The current learning rate: 0.00010\n",
      "Epoch 10000: train loss 3.3453, val loss 3.3457\n",
      "The current learning rate: 0.00010\n",
      "Epoch 10500: train loss 3.3091, val loss 3.3135\n",
      "The current learning rate: 0.00010\n",
      "Epoch 11000: train loss 3.2833, val loss 3.2790\n",
      "The current learning rate: 0.00010\n",
      "Epoch 11500: train loss 3.2497, val loss 3.2658\n",
      "The current learning rate: 0.00010\n",
      "Epoch 12000: train loss 3.2325, val loss 3.2324\n",
      "The current learning rate: 0.00011\n",
      "Epoch 12500: train loss 3.2145, val loss 3.2110\n",
      "The current learning rate: 0.00011\n",
      "Epoch 13000: train loss 3.1863, val loss 3.1920\n",
      "The current learning rate: 0.00011\n",
      "Epoch 13500: train loss 3.1643, val loss 3.1719\n",
      "The current learning rate: 0.00011\n",
      "Epoch 14000: train loss 3.1494, val loss 3.1495\n",
      "The current learning rate: 0.00011\n",
      "Epoch 14500: train loss 3.1327, val loss 3.1266\n",
      "The current learning rate: 0.00011\n",
      "Epoch 15000: train loss 3.1173, val loss 3.1093\n",
      "The current learning rate: 0.00011\n",
      "Epoch 15500: train loss 3.0929, val loss 3.0940\n",
      "The current learning rate: 0.00011\n",
      "Epoch 16000: train loss 3.0778, val loss 3.0771\n",
      "The current learning rate: 0.00011\n",
      "Epoch 16500: train loss 3.0652, val loss 3.0645\n",
      "The current learning rate: 0.00011\n",
      "Epoch 17000: train loss 3.0459, val loss 3.0502\n",
      "The current learning rate: 0.00011\n",
      "Epoch 17500: train loss 3.0335, val loss 3.0293\n",
      "The current learning rate: 0.00011\n",
      "Epoch 18000: train loss 3.0195, val loss 3.0249\n",
      "The current learning rate: 0.00011\n",
      "Epoch 18500: train loss 3.0029, val loss 3.0089\n",
      "The current learning rate: 0.00011\n",
      "Epoch 19000: train loss 2.9995, val loss 2.9863\n",
      "The current learning rate: 0.00011\n",
      "Epoch 19500: train loss 2.9800, val loss 2.9795\n",
      "The current learning rate: 0.00012\n",
      "Epoch 20000: train loss 2.9696, val loss 2.9682\n",
      "The current learning rate: 0.00012\n",
      "Epoch 20500: train loss 2.9572, val loss 2.9612\n",
      "The current learning rate: 0.00012\n",
      "Epoch 21000: train loss 2.9480, val loss 2.9537\n",
      "The current learning rate: 0.00012\n",
      "Epoch 21500: train loss 2.9329, val loss 2.9360\n",
      "The current learning rate: 0.00012\n",
      "Epoch 22000: train loss 2.9210, val loss 2.9245\n",
      "The current learning rate: 0.00012\n",
      "Epoch 22500: train loss 2.9115, val loss 2.9094\n",
      "The current learning rate: 0.00012\n",
      "Epoch 23000: train loss 2.9009, val loss 2.9052\n",
      "The current learning rate: 0.00012\n",
      "Epoch 23500: train loss 2.8930, val loss 2.9067\n",
      "The current learning rate: 0.00012\n",
      "Epoch 24000: train loss 2.8803, val loss 2.8808\n",
      "The current learning rate: 0.00012\n",
      "Epoch 24500: train loss 2.8678, val loss 2.8662\n",
      "The current learning rate: 0.00012\n",
      "Epoch 25000: train loss 2.8653, val loss 2.8534\n",
      "The current learning rate: 0.00013\n",
      "Epoch 25500: train loss 2.8532, val loss 2.8593\n",
      "The current learning rate: 0.00013\n",
      "Epoch 26000: train loss 2.8384, val loss 2.8476\n",
      "The current learning rate: 0.00013\n",
      "Epoch 26500: train loss 2.8221, val loss 2.8365\n",
      "The current learning rate: 0.00013\n",
      "Epoch 27000: train loss 2.8272, val loss 2.8210\n",
      "The current learning rate: 0.00013\n",
      "Epoch 27500: train loss 2.8151, val loss 2.8112\n",
      "The current learning rate: 0.00013\n",
      "Epoch 28000: train loss 2.8009, val loss 2.8001\n",
      "The current learning rate: 0.00013\n",
      "Epoch 28500: train loss 2.7858, val loss 2.7978\n",
      "The current learning rate: 0.00013\n",
      "Epoch 29000: train loss 2.7769, val loss 2.7801\n",
      "The current learning rate: 0.00013\n",
      "Epoch 29500: train loss 2.7782, val loss 2.7705\n",
      "The current learning rate: 0.00014\n",
      "Epoch 30000: train loss 2.7696, val loss 2.7741\n",
      "The current learning rate: 0.00014\n",
      "Epoch 30500: train loss 2.7458, val loss 2.7606\n",
      "The current learning rate: 0.00014\n",
      "Epoch 31000: train loss 2.7474, val loss 2.7454\n",
      "The current learning rate: 0.00014\n",
      "Epoch 31500: train loss 2.7450, val loss 2.7534\n",
      "The current learning rate: 0.00014\n",
      "Epoch 32000: train loss 2.7323, val loss 2.7375\n",
      "The current learning rate: 0.00014\n",
      "Epoch 32500: train loss 2.7219, val loss 2.7259\n",
      "The current learning rate: 0.00014\n",
      "Epoch 33000: train loss 2.7093, val loss 2.7135\n",
      "The current learning rate: 0.00014\n",
      "Epoch 33500: train loss 2.7034, val loss 2.7063\n",
      "The current learning rate: 0.00015\n",
      "Epoch 34000: train loss 2.6937, val loss 2.6964\n",
      "The current learning rate: 0.00015\n",
      "Epoch 34500: train loss 2.6868, val loss 2.6977\n",
      "The current learning rate: 0.00015\n",
      "Epoch 35000: train loss 2.6853, val loss 2.6892\n",
      "The current learning rate: 0.00015\n",
      "Epoch 35500: train loss 2.6680, val loss 2.6829\n",
      "The current learning rate: 0.00015\n",
      "Epoch 36000: train loss 2.6647, val loss 2.6677\n",
      "The current learning rate: 0.00015\n",
      "Epoch 36500: train loss 2.6620, val loss 2.6535\n",
      "The current learning rate: 0.00015\n",
      "Epoch 37000: train loss 2.6598, val loss 2.6550\n",
      "The current learning rate: 0.00015\n",
      "Epoch 37500: train loss 2.6392, val loss 2.6428\n",
      "The current learning rate: 0.00016\n",
      "Epoch 38000: train loss 2.6350, val loss 2.6386\n",
      "The current learning rate: 0.00016\n",
      "Epoch 38500: train loss 2.6359, val loss 2.6318\n",
      "The current learning rate: 0.00016\n",
      "Epoch 39000: train loss 2.6163, val loss 2.6163\n",
      "The current learning rate: 0.00016\n",
      "Epoch 39500: train loss 2.6093, val loss 2.6109\n",
      "The current learning rate: 0.00016\n",
      "Epoch 40000: train loss 2.6041, val loss 2.5976\n",
      "The current learning rate: 0.00016\n",
      "Epoch 40500: train loss 2.5969, val loss 2.5853\n",
      "The current learning rate: 0.00017\n",
      "Epoch 41000: train loss 2.5850, val loss 2.5901\n",
      "The current learning rate: 0.00017\n",
      "Epoch 41500: train loss 2.5839, val loss 2.5841\n",
      "The current learning rate: 0.00017\n",
      "Epoch 42000: train loss 2.5726, val loss 2.5720\n",
      "The current learning rate: 0.00017\n",
      "Epoch 42500: train loss 2.5614, val loss 2.5725\n",
      "The current learning rate: 0.00017\n",
      "Epoch 43000: train loss 2.5426, val loss 2.5541\n",
      "The current learning rate: 0.00017\n",
      "Epoch 43500: train loss 2.5398, val loss 2.5490\n",
      "The current learning rate: 0.00018\n",
      "Epoch 44000: train loss 2.5361, val loss 2.5329\n",
      "The current learning rate: 0.00018\n",
      "Epoch 44500: train loss 2.5329, val loss 2.5350\n",
      "The current learning rate: 0.00018\n",
      "Epoch 45000: train loss 2.5170, val loss 2.5203\n",
      "The current learning rate: 0.00018\n",
      "Epoch 45500: train loss 2.5184, val loss 2.5151\n",
      "The current learning rate: 0.00018\n",
      "Epoch 46000: train loss 2.5076, val loss 2.5072\n",
      "The current learning rate: 0.00018\n",
      "Epoch 46500: train loss 2.4940, val loss 2.5058\n",
      "The current learning rate: 0.00019\n",
      "Epoch 47000: train loss 2.4920, val loss 2.4907\n",
      "The current learning rate: 0.00019\n",
      "Epoch 47500: train loss 2.4913, val loss 2.4818\n",
      "The current learning rate: 0.00019\n",
      "Epoch 48000: train loss 2.4781, val loss 2.4803\n",
      "The current learning rate: 0.00019\n",
      "Epoch 48500: train loss 2.4673, val loss 2.4642\n",
      "The current learning rate: 0.00019\n",
      "Epoch 49000: train loss 2.4611, val loss 2.4714\n",
      "The current learning rate: 0.00019\n",
      "Epoch 49500: train loss 2.4433, val loss 2.4527\n",
      "The current learning rate: 0.00020\n",
      "Epoch 50000: train loss 2.4405, val loss 2.4473\n",
      "The current learning rate: 0.00020\n",
      "Epoch 50500: train loss 2.4382, val loss 2.4478\n",
      "The current learning rate: 0.00020\n",
      "Epoch 51000: train loss 2.4301, val loss 2.4315\n",
      "The current learning rate: 0.00020\n",
      "Epoch 51500: train loss 2.4271, val loss 2.4280\n",
      "The current learning rate: 0.00020\n",
      "Epoch 52000: train loss 2.4116, val loss 2.4155\n",
      "The current learning rate: 0.00020\n",
      "Epoch 52500: train loss 2.4142, val loss 2.4119\n",
      "The current learning rate: 0.00021\n",
      "Epoch 53000: train loss 2.3996, val loss 2.3968\n",
      "The current learning rate: 0.00021\n",
      "Epoch 53500: train loss 2.3865, val loss 2.3957\n",
      "The current learning rate: 0.00021\n",
      "Epoch 54000: train loss 2.3861, val loss 2.3868\n",
      "The current learning rate: 0.00021\n",
      "Epoch 54500: train loss 2.3798, val loss 2.3747\n",
      "The current learning rate: 0.00021\n",
      "Epoch 55000: train loss 2.3638, val loss 2.3699\n",
      "The current learning rate: 0.00022\n",
      "Epoch 55500: train loss 2.3660, val loss 2.3643\n",
      "The current learning rate: 0.00022\n",
      "Epoch 56000: train loss 2.3568, val loss 2.3636\n",
      "The current learning rate: 0.00022\n",
      "Epoch 56500: train loss 2.3450, val loss 2.3472\n",
      "The current learning rate: 0.00022\n",
      "Epoch 57000: train loss 2.3426, val loss 2.3488\n",
      "The current learning rate: 0.00022\n",
      "Epoch 57500: train loss 2.3235, val loss 2.3319\n",
      "The current learning rate: 0.00023\n",
      "Epoch 58000: train loss 2.3223, val loss 2.3268\n",
      "The current learning rate: 0.00023\n",
      "Epoch 58500: train loss 2.3121, val loss 2.3165\n",
      "The current learning rate: 0.00023\n",
      "Epoch 59000: train loss 2.3103, val loss 2.3142\n",
      "The current learning rate: 0.00023\n",
      "Epoch 59500: train loss 2.2950, val loss 2.3023\n",
      "The current learning rate: 0.00023\n",
      "Epoch 60000: train loss 2.2857, val loss 2.3020\n",
      "The current learning rate: 0.00024\n",
      "Epoch 60500: train loss 2.2831, val loss 2.2890\n",
      "The current learning rate: 0.00024\n",
      "Epoch 61000: train loss 2.2796, val loss 2.2842\n",
      "The current learning rate: 0.00024\n",
      "Epoch 61500: train loss 2.2682, val loss 2.2775\n",
      "The current learning rate: 0.00024\n",
      "Epoch 62000: train loss 2.2693, val loss 2.2751\n",
      "The current learning rate: 0.00024\n",
      "Epoch 62500: train loss 2.2611, val loss 2.2675\n",
      "The current learning rate: 0.00025\n",
      "Epoch 63000: train loss 2.2462, val loss 2.2590\n",
      "The current learning rate: 0.00025\n",
      "Epoch 63500: train loss 2.2399, val loss 2.2522\n",
      "The current learning rate: 0.00025\n",
      "Epoch 64000: train loss 2.2406, val loss 2.2434\n",
      "The current learning rate: 0.00025\n",
      "Epoch 64500: train loss 2.2297, val loss 2.2371\n",
      "The current learning rate: 0.00025\n",
      "Epoch 65000: train loss 2.2237, val loss 2.2294\n",
      "The current learning rate: 0.00026\n",
      "Epoch 65500: train loss 2.2181, val loss 2.2229\n",
      "The current learning rate: 0.00026\n",
      "Epoch 66000: train loss 2.2006, val loss 2.2142\n",
      "The current learning rate: 0.00026\n",
      "Epoch 66500: train loss 2.2032, val loss 2.2134\n",
      "The current learning rate: 0.00026\n",
      "Epoch 67000: train loss 2.2025, val loss 2.2041\n",
      "The current learning rate: 0.00026\n",
      "Epoch 67500: train loss 2.1927, val loss 2.1953\n",
      "The current learning rate: 0.00027\n",
      "Epoch 68000: train loss 2.1831, val loss 2.1914\n",
      "The current learning rate: 0.00027\n",
      "Epoch 68500: train loss 2.1735, val loss 2.1796\n",
      "The current learning rate: 0.00027\n",
      "Epoch 69000: train loss 2.1651, val loss 2.1725\n",
      "The current learning rate: 0.00027\n",
      "Epoch 69500: train loss 2.1684, val loss 2.1746\n",
      "The current learning rate: 0.00027\n",
      "Epoch 70000: train loss 2.1525, val loss 2.1701\n",
      "The current learning rate: 0.00028\n",
      "Epoch 70500: train loss 2.1512, val loss 2.1582\n",
      "The current learning rate: 0.00028\n",
      "Epoch 71000: train loss 2.1455, val loss 2.1512\n",
      "The current learning rate: 0.00028\n",
      "Epoch 71500: train loss 2.1420, val loss 2.1457\n",
      "The current learning rate: 0.00028\n",
      "Epoch 72000: train loss 2.1343, val loss 2.1372\n",
      "The current learning rate: 0.00029\n",
      "Epoch 72500: train loss 2.1271, val loss 2.1287\n",
      "The current learning rate: 0.00029\n",
      "Epoch 73000: train loss 2.1178, val loss 2.1271\n",
      "The current learning rate: 0.00029\n",
      "Epoch 73500: train loss 2.1167, val loss 2.1183\n",
      "The current learning rate: 0.00029\n",
      "Epoch 74000: train loss 2.1072, val loss 2.1059\n",
      "The current learning rate: 0.00029\n",
      "Epoch 74500: train loss 2.1048, val loss 2.1113\n",
      "The current learning rate: 0.00030\n",
      "Epoch 75000: train loss 2.0958, val loss 2.1070\n",
      "The current learning rate: 0.00030\n",
      "Epoch 75500: train loss 2.0961, val loss 2.0931\n",
      "The current learning rate: 0.00030\n",
      "Epoch 76000: train loss 2.0816, val loss 2.0931\n",
      "The current learning rate: 0.00030\n",
      "Epoch 76500: train loss 2.0777, val loss 2.0785\n",
      "The current learning rate: 0.00030\n",
      "Epoch 77000: train loss 2.0748, val loss 2.0763\n",
      "The current learning rate: 0.00031\n",
      "Epoch 77500: train loss 2.0732, val loss 2.0778\n",
      "The current learning rate: 0.00031\n",
      "Epoch 78000: train loss 2.0596, val loss 2.0598\n",
      "The current learning rate: 0.00031\n",
      "Epoch 78500: train loss 2.0549, val loss 2.0588\n",
      "The current learning rate: 0.00031\n",
      "Epoch 79000: train loss 2.0575, val loss 2.0653\n",
      "The current learning rate: 0.00031\n",
      "Epoch 79500: train loss 2.0479, val loss 2.0592\n",
      "The current learning rate: 0.00032\n",
      "Epoch 80000: train loss 2.0406, val loss 2.0527\n",
      "The current learning rate: 0.00032\n",
      "Epoch 80500: train loss 2.0335, val loss 2.0396\n",
      "The current learning rate: 0.00032\n",
      "Epoch 81000: train loss 2.0341, val loss 2.0317\n",
      "The current learning rate: 0.00032\n",
      "Epoch 81500: train loss 2.0279, val loss 2.0276\n",
      "The current learning rate: 0.00033\n",
      "Epoch 82000: train loss 2.0167, val loss 2.0283\n",
      "The current learning rate: 0.00033\n",
      "Epoch 82500: train loss 2.0099, val loss 2.0144\n",
      "The current learning rate: 0.00033\n",
      "Epoch 83000: train loss 2.0061, val loss 2.0172\n",
      "The current learning rate: 0.00033\n",
      "Epoch 83500: train loss 1.9992, val loss 2.0127\n",
      "The current learning rate: 0.00033\n",
      "Epoch 84000: train loss 2.0024, val loss 2.0170\n",
      "The current learning rate: 0.00034\n",
      "Epoch 84500: train loss 1.9920, val loss 1.9976\n",
      "The current learning rate: 0.00034\n",
      "Epoch 85000: train loss 1.9853, val loss 2.0026\n",
      "The current learning rate: 0.00034\n",
      "Epoch 85500: train loss 1.9812, val loss 1.9970\n",
      "The current learning rate: 0.00034\n",
      "Epoch 86000: train loss 1.9786, val loss 1.9858\n",
      "The current learning rate: 0.00034\n",
      "Epoch 86500: train loss 1.9737, val loss 1.9770\n",
      "The current learning rate: 0.00035\n",
      "Epoch 87000: train loss 1.9656, val loss 1.9736\n",
      "The current learning rate: 0.00035\n",
      "Epoch 87500: train loss 1.9694, val loss 1.9662\n",
      "The current learning rate: 0.00035\n",
      "Epoch 88000: train loss 1.9631, val loss 1.9633\n",
      "The current learning rate: 0.00035\n",
      "Epoch 88500: train loss 1.9548, val loss 1.9653\n",
      "The current learning rate: 0.00035\n",
      "Epoch 89000: train loss 1.9489, val loss 1.9574\n",
      "The current learning rate: 0.00036\n",
      "Epoch 89500: train loss 1.9498, val loss 1.9570\n",
      "The current learning rate: 0.00036\n",
      "Epoch 90000: train loss 1.9456, val loss 1.9550\n",
      "The current learning rate: 0.00036\n",
      "Epoch 90500: train loss 1.9383, val loss 1.9456\n",
      "The current learning rate: 0.00036\n",
      "Epoch 91000: train loss 1.9340, val loss 1.9471\n",
      "The current learning rate: 0.00036\n",
      "Epoch 91500: train loss 1.9265, val loss 1.9364\n",
      "The current learning rate: 0.00037\n",
      "Epoch 92000: train loss 1.9232, val loss 1.9444\n",
      "The current learning rate: 0.00037\n",
      "Epoch 92500: train loss 1.9224, val loss 1.9378\n",
      "The current learning rate: 0.00037\n",
      "Epoch 93000: train loss 1.9161, val loss 1.9228\n",
      "The current learning rate: 0.00037\n",
      "Epoch 93500: train loss 1.9132, val loss 1.9256\n",
      "The current learning rate: 0.00037\n",
      "Epoch 94000: train loss 1.9065, val loss 1.9176\n",
      "The current learning rate: 0.00038\n",
      "Epoch 94500: train loss 1.9044, val loss 1.9135\n",
      "The current learning rate: 0.00038\n",
      "Epoch 95000: train loss 1.8958, val loss 1.9113\n",
      "The current learning rate: 0.00038\n",
      "Epoch 95500: train loss 1.8894, val loss 1.9122\n",
      "The current learning rate: 0.00038\n",
      "Epoch 96000: train loss 1.8963, val loss 1.9074\n",
      "The current learning rate: 0.00038\n",
      "Epoch 96500: train loss 1.8908, val loss 1.8990\n",
      "The current learning rate: 0.00039\n",
      "Epoch 97000: train loss 1.8857, val loss 1.8892\n",
      "The current learning rate: 0.00039\n",
      "Epoch 97500: train loss 1.8747, val loss 1.8975\n",
      "The current learning rate: 0.00039\n",
      "Epoch 98000: train loss 1.8815, val loss 1.8876\n",
      "The current learning rate: 0.00039\n",
      "Epoch 98500: train loss 1.8763, val loss 1.8831\n",
      "The current learning rate: 0.00039\n",
      "Epoch 99000: train loss 1.8662, val loss 1.8781\n",
      "The current learning rate: 0.00040\n",
      "Epoch 99500: train loss 1.8775, val loss 1.8744\n",
      "The current learning rate: 0.00040\n",
      "Epoch 100000: train loss 1.8650, val loss 1.8695\n",
      "The current learning rate: 0.00040\n",
      "Epoch 100500: train loss 1.8573, val loss 1.8745\n",
      "The current learning rate: 0.00040\n",
      "Epoch 101000: train loss 1.8591, val loss 1.8694\n",
      "The current learning rate: 0.00040\n",
      "Epoch 101500: train loss 1.8543, val loss 1.8651\n",
      "The current learning rate: 0.00040\n",
      "Epoch 102000: train loss 1.8507, val loss 1.8682\n",
      "The current learning rate: 0.00041\n",
      "Epoch 102500: train loss 1.8529, val loss 1.8583\n",
      "The current learning rate: 0.00041\n",
      "Epoch 103000: train loss 1.8468, val loss 1.8620\n",
      "The current learning rate: 0.00041\n",
      "Epoch 103500: train loss 1.8453, val loss 1.8510\n",
      "The current learning rate: 0.00041\n",
      "Epoch 104000: train loss 1.8361, val loss 1.8461\n",
      "The current learning rate: 0.00041\n",
      "Epoch 104500: train loss 1.8332, val loss 1.8556\n",
      "The current learning rate: 0.00041\n",
      "Epoch 105000: train loss 1.8310, val loss 1.8415\n",
      "The current learning rate: 0.00042\n",
      "Epoch 105500: train loss 1.8321, val loss 1.8380\n",
      "The current learning rate: 0.00042\n",
      "Epoch 106000: train loss 1.8247, val loss 1.8381\n",
      "The current learning rate: 0.00042\n",
      "Epoch 106500: train loss 1.8227, val loss 1.8369\n",
      "The current learning rate: 0.00042\n",
      "Epoch 107000: train loss 1.8261, val loss 1.8336\n",
      "The current learning rate: 0.00042\n",
      "Epoch 107500: train loss 1.8187, val loss 1.8325\n",
      "The current learning rate: 0.00042\n",
      "Epoch 108000: train loss 1.8175, val loss 1.8262\n",
      "The current learning rate: 0.00043\n",
      "Epoch 108500: train loss 1.8079, val loss 1.8268\n",
      "The current learning rate: 0.00043\n",
      "Epoch 109000: train loss 1.8088, val loss 1.8171\n",
      "The current learning rate: 0.00043\n",
      "Epoch 109500: train loss 1.8019, val loss 1.8191\n",
      "The current learning rate: 0.00043\n",
      "Epoch 110000: train loss 1.8074, val loss 1.8164\n",
      "The current learning rate: 0.00043\n",
      "Epoch 110500: train loss 1.8070, val loss 1.8192\n",
      "The current learning rate: 0.00043\n",
      "Epoch 111000: train loss 1.7978, val loss 1.8149\n",
      "The current learning rate: 0.00044\n",
      "Epoch 111500: train loss 1.7951, val loss 1.8104\n",
      "The current learning rate: 0.00044\n",
      "Epoch 112000: train loss 1.7896, val loss 1.8088\n",
      "The current learning rate: 0.00044\n",
      "Epoch 112500: train loss 1.7970, val loss 1.8106\n",
      "The current learning rate: 0.00044\n",
      "Epoch 113000: train loss 1.7921, val loss 1.8088\n",
      "The current learning rate: 0.00044\n",
      "Epoch 113500: train loss 1.7801, val loss 1.8035\n",
      "The current learning rate: 0.00044\n",
      "Epoch 114000: train loss 1.7756, val loss 1.7865\n",
      "The current learning rate: 0.00045\n",
      "Epoch 114500: train loss 1.7782, val loss 1.7973\n",
      "The current learning rate: 0.00045\n",
      "Epoch 115000: train loss 1.7843, val loss 1.7937\n",
      "The current learning rate: 0.00045\n",
      "Epoch 115500: train loss 1.7769, val loss 1.7958\n",
      "The current learning rate: 0.00045\n",
      "Epoch 116000: train loss 1.7741, val loss 1.7841\n",
      "The current learning rate: 0.00045\n",
      "Epoch 116500: train loss 1.7680, val loss 1.7841\n",
      "The current learning rate: 0.00045\n",
      "Epoch 117000: train loss 1.7685, val loss 1.7796\n",
      "The current learning rate: 0.00045\n",
      "Epoch 117500: train loss 1.7643, val loss 1.7770\n",
      "The current learning rate: 0.00045\n",
      "Epoch 118000: train loss 1.7633, val loss 1.7764\n",
      "The current learning rate: 0.00046\n",
      "Epoch 118500: train loss 1.7664, val loss 1.7820\n",
      "The current learning rate: 0.00046\n",
      "Epoch 119000: train loss 1.7611, val loss 1.7757\n",
      "The current learning rate: 0.00046\n",
      "Epoch 119500: train loss 1.7593, val loss 1.7781\n",
      "The current learning rate: 0.00046\n",
      "Epoch 120000: train loss 1.7538, val loss 1.7671\n",
      "The current learning rate: 0.00046\n",
      "Epoch 120500: train loss 1.7472, val loss 1.7677\n",
      "The current learning rate: 0.00046\n",
      "Epoch 121000: train loss 1.7459, val loss 1.7610\n",
      "The current learning rate: 0.00046\n",
      "Epoch 121500: train loss 1.7496, val loss 1.7640\n",
      "The current learning rate: 0.00046\n",
      "Epoch 122000: train loss 1.7481, val loss 1.7711\n",
      "The current learning rate: 0.00047\n",
      "Epoch 122500: train loss 1.7492, val loss 1.7598\n",
      "The current learning rate: 0.00047\n",
      "Epoch 123000: train loss 1.7501, val loss 1.7578\n",
      "The current learning rate: 0.00047\n",
      "Epoch 123500: train loss 1.7436, val loss 1.7558\n",
      "The current learning rate: 0.00047\n",
      "Epoch 124000: train loss 1.7385, val loss 1.7523\n",
      "The current learning rate: 0.00047\n",
      "Epoch 124500: train loss 1.7393, val loss 1.7534\n",
      "The current learning rate: 0.00047\n",
      "Epoch 125000: train loss 1.7373, val loss 1.7472\n",
      "The current learning rate: 0.00047\n",
      "Epoch 125500: train loss 1.7303, val loss 1.7481\n",
      "The current learning rate: 0.00047\n",
      "Epoch 126000: train loss 1.7268, val loss 1.7510\n",
      "The current learning rate: 0.00047\n",
      "Epoch 126500: train loss 1.7291, val loss 1.7478\n",
      "The current learning rate: 0.00048\n",
      "Epoch 127000: train loss 1.7276, val loss 1.7480\n",
      "The current learning rate: 0.00048\n",
      "Epoch 127500: train loss 1.7255, val loss 1.7437\n",
      "The current learning rate: 0.00048\n",
      "Epoch 128000: train loss 1.7250, val loss 1.7378\n",
      "The current learning rate: 0.00048\n",
      "Epoch 128500: train loss 1.7272, val loss 1.7381\n",
      "The current learning rate: 0.00048\n",
      "Epoch 129000: train loss 1.7108, val loss 1.7366\n",
      "The current learning rate: 0.00048\n",
      "Epoch 129500: train loss 1.7165, val loss 1.7391\n",
      "The current learning rate: 0.00048\n",
      "Epoch 130000: train loss 1.7142, val loss 1.7314\n",
      "The current learning rate: 0.00048\n",
      "Epoch 130500: train loss 1.7105, val loss 1.7271\n",
      "The current learning rate: 0.00048\n",
      "Epoch 131000: train loss 1.7088, val loss 1.7259\n",
      "The current learning rate: 0.00048\n",
      "Epoch 131500: train loss 1.7064, val loss 1.7328\n",
      "The current learning rate: 0.00048\n",
      "Epoch 132000: train loss 1.7019, val loss 1.7257\n",
      "The current learning rate: 0.00049\n",
      "Epoch 132500: train loss 1.7062, val loss 1.7280\n",
      "The current learning rate: 0.00049\n",
      "Epoch 133000: train loss 1.7021, val loss 1.7221\n",
      "The current learning rate: 0.00049\n",
      "Epoch 133500: train loss 1.7046, val loss 1.7181\n",
      "The current learning rate: 0.00049\n",
      "Epoch 134000: train loss 1.6968, val loss 1.7160\n",
      "The current learning rate: 0.00049\n",
      "Epoch 134500: train loss 1.6993, val loss 1.7189\n",
      "The current learning rate: 0.00049\n",
      "Epoch 135000: train loss 1.6945, val loss 1.7214\n",
      "The current learning rate: 0.00049\n",
      "Epoch 135500: train loss 1.6994, val loss 1.7159\n",
      "The current learning rate: 0.00049\n",
      "Epoch 136000: train loss 1.7001, val loss 1.7118\n",
      "The current learning rate: 0.00049\n",
      "Epoch 136500: train loss 1.6916, val loss 1.7155\n",
      "The current learning rate: 0.00049\n",
      "Epoch 137000: train loss 1.6914, val loss 1.7075\n",
      "The current learning rate: 0.00049\n",
      "Epoch 137500: train loss 1.6923, val loss 1.7027\n",
      "The current learning rate: 0.00049\n",
      "Epoch 138000: train loss 1.6880, val loss 1.7013\n",
      "The current learning rate: 0.00049\n",
      "Epoch 138500: train loss 1.6788, val loss 1.7059\n",
      "The current learning rate: 0.00049\n",
      "Epoch 139000: train loss 1.6875, val loss 1.7032\n",
      "The current learning rate: 0.00049\n",
      "Epoch 139500: train loss 1.6870, val loss 1.7001\n",
      "The current learning rate: 0.00050\n",
      "Epoch 140000: train loss 1.6827, val loss 1.7034\n",
      "The current learning rate: 0.00050\n",
      "Epoch 140500: train loss 1.6777, val loss 1.6932\n",
      "The current learning rate: 0.00050\n",
      "Epoch 141000: train loss 1.6713, val loss 1.6996\n",
      "The current learning rate: 0.00050\n",
      "Epoch 141500: train loss 1.6734, val loss 1.7001\n",
      "The current learning rate: 0.00050\n",
      "Epoch 142000: train loss 1.6737, val loss 1.6943\n",
      "The current learning rate: 0.00050\n",
      "Epoch 142500: train loss 1.6708, val loss 1.6900\n",
      "The current learning rate: 0.00050\n",
      "Epoch 143000: train loss 1.6678, val loss 1.6864\n",
      "The current learning rate: 0.00050\n",
      "Epoch 143500: train loss 1.6684, val loss 1.6879\n",
      "The current learning rate: 0.00050\n",
      "Epoch 144000: train loss 1.6685, val loss 1.6878\n",
      "The current learning rate: 0.00050\n",
      "Epoch 144500: train loss 1.6674, val loss 1.6875\n",
      "The current learning rate: 0.00050\n",
      "Epoch 145000: train loss 1.6606, val loss 1.6842\n",
      "The current learning rate: 0.00050\n",
      "Epoch 145500: train loss 1.6656, val loss 1.6876\n",
      "The current learning rate: 0.00050\n",
      "Epoch 146000: train loss 1.6633, val loss 1.6812\n",
      "The current learning rate: 0.00050\n",
      "Epoch 146500: train loss 1.6617, val loss 1.6794\n",
      "The current learning rate: 0.00050\n",
      "Epoch 147000: train loss 1.6622, val loss 1.6820\n",
      "The current learning rate: 0.00050\n",
      "Epoch 147500: train loss 1.6548, val loss 1.6770\n",
      "The current learning rate: 0.00050\n",
      "Epoch 148000: train loss 1.6564, val loss 1.6769\n",
      "The current learning rate: 0.00050\n",
      "Epoch 148500: train loss 1.6486, val loss 1.6765\n",
      "The current learning rate: 0.00050\n",
      "Epoch 149000: train loss 1.6486, val loss 1.6732\n",
      "The current learning rate: 0.00050\n",
      "Epoch 149500: train loss 1.6508, val loss 1.6673\n",
      "The current learning rate: 0.00050\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdzhSo_7TcgI"
   },
   "source": [
    "## Step 9: Plot the SLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "KSWpvAnakjK6",
    "outputId": "78d59ad2-555a-4088-bdb9-4b98485bc86b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2ocyyQwkjK6"
   },
   "source": [
    "## Step 10: Run SLM Inference on our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06NrdWKdkjK7",
    "outputId": "6173d153-cdee-4b00-c634-4a27e133e641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8PgWXb-kjK7",
    "outputId": "2b01a0fd-eb9f-4016-9cd9-e91984eebcf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin. This pumpkin was very special. It was a big special pumpkin because it was real. Everyone was so amazed by the green color!\n",
      "\n",
      "One day, a hurricane came to the field and it was strong. The wind was so strong that it broke the field and scared away the whole trees. All the pumpkins were safe.\n",
      "\n",
      "They all wanted to play near the patch, so they stepped closer. The goodies inside were a lot of fruit and fruit. They played games and laughed with each other.\n",
      "\n",
      "But then they heard a loud noise. It was the sound of war - a dangerous animal and it was coming from behind the field. They sat in their spot and watched the birds move in the sky.\n",
      "\n",
      "The powerful animal and the other animals in the field were scared of the dangerous wild animal too. After a while, the scary animal removed the other animals and made their homes all ok. The harsh animal was very happy and they had become friends. Now the field could\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZsL_mONRZAO",
    "outputId": "0e61e355-a5f8-4d38-b8ed-3e65e6bc1414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A little girl went to the woods alone. She looked around and the animals had been playing a game. Then she said, \"I want to tell you a secret.\"\n",
      "\n",
      "The little girl was surprised at the little animals' idea. She quickly began to tell a funny joke. After a few moments, the animal revealed a perfect little bird. The little girl smiled. She said, \"Very good!\"\n",
      "\n",
      "The little girl smiled too. She waved goodbye to the animals, happy with her creation. She said, \"We have a new friend!\"\n",
      "\n",
      "The animal replied, \"Now is this place!\"The sun shone brightly and the birds sang and pecked at the ground. Jill was delighted to see her pet, Pippy, running around in the woods. boardinginnieorting made her happy and powerful.\n",
      "\n",
      "Suddenly she heard a noise. Jill looked up and discovered a big grey rabbit. It had one beautifully gave her an hugs. Jill was so happy to see the rabbit again.\n",
      "\n",
      "She smiled\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A little girl went to the woods\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xu0MOoxmf6oo",
    "outputId": "3a6b0ade-d2a1-478a-d988-764061c85589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The footballer was pleased with the idea one day. From then on, the family decided to accept the idea we created together and value it more than ever.Tommy was a 3 year old boy who loved playing with his toys. He kept playing every day in his garden. One day he decided to take a walk and he saw a helpless animal walking through the bridge. He wanted to help it, but he knew he had to measure it!\n",
      "\n",
      "He ran to the bridge and peered in the water. He saw a frog. The frog was helpless and couldn't jump high enough. Benny knew he had to help. He bent down, grabbed the frog in his hands and dipped it into the water. With a soft heave, the little frog was able to out its improved log.\n",
      "\n",
      "From that day on, Benny was able to make sure that the hippo was deserved and taken care of. The moral of the story is that when you help others, everyone can accomplish anything.One day\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The footballer was\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
