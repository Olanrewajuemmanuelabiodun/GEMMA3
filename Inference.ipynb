{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S29r1WEoAdqT",
    "outputId": "cf18bc7b-ae8d-468a-ca5e-8d0e60e87a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub tiktoken torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "f5c7edae56e14a11b3e05ac5582ebbe7",
      "67dc19a1ae724104ac374dda561eaef8",
      "b50395fd43354fc7bbc0cf7b09108f07",
      "9c22ee198eeb47e8a16d063a041579da",
      "0c84975542904b8f8842f4d6dba3d55b",
      "2812c85ff15d41b49df074addbd4dafc",
      "1a0f669281b548a2ac753672d5ae2843",
      "8e0b5dd086144ed6a9c6529b30b850e6",
      "747fea7a6b5e49aa93520d5eaba256b2",
      "a6db3e5df8344659a03d37f84d3b6f62",
      "de6aab24fcec4a989d7f772d3a05e883"
     ]
    },
    "id": "0VSgXdFTAsgh",
    "outputId": "ad6a3692-5975-4ab9-fc3f-4bf4a152c427"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c7edae56e14a11b3e05ac5582ebbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_gemma3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--EmmanuelOlanrewaju--gemma3-model/snapshots/9587263ebc374e64f81ceb77e3d35e2a9115833a/modeling_gemma3.py\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"EmmanuelOlanrewaju/gemma3-model\"\n",
    "\n",
    "\n",
    "modeling_path = hf_hub_download(repo_id, \"modeling_gemma3.py\")\n",
    "print(modeling_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LCC89SpMAxlh"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"modeling_gemma3\", modeling_path)\n",
    "modeling_gemma3 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(modeling_gemma3)\n",
    "\n",
    "\n",
    "Gemma3Model = modeling_gemma3.Gemma3Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "8fb12f0185fd4d9dac7143d3e96e152b",
      "9dfbafd707c844079fb68d85b710e42c",
      "d92827221e96416ea46f5319ab3ff40b",
      "00f31f5389364246bfe60afa2329d130",
      "1d47ec9fa67d43db935606e7a25b280e",
      "d3ff36136c5647d58f3781eb3319bb5a",
      "775410b9d97c46188e02ecd1e2f7779e",
      "62f19f69c7ac4df59db41cef6ecc6857",
      "36125b855937482484fbfd35d4381ab0",
      "c566f042718d4bca8347f594d0052865",
      "999b64993af54357b2f9ba85403b9fce",
      "522db905ea9c4a4aafe68931c0507b59",
      "8395b6623b9f46c9b5497250971ebcc6",
      "6f35979b1eae446ea7faacbbab80cd3f",
      "ae6a0854994b432ea283f4af7a73f598",
      "e19a8ef723d643a6b5a5d508e104dc7c",
      "67ff491cda3f401d9de58290ac67492b",
      "0df56734a573450eba2420c72ae6c5cb",
      "f7aa9a6e3360405eb6a61355d74528bc",
      "a5c1cd31f4f14610a25495f370dbdc1b",
      "f2b500b3ff9341d3887fc980e5a348f9",
      "1763f6e9c30547e8b03b4da5b8d242ca"
     ]
    },
    "id": "zLuNuCVvBD4z",
    "outputId": "272312e3-b987-469c-f94a-fe57143732c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb12f0185fd4d9dac7143d3e96e152b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/784 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522db905ea9c4a4aafe68931c0507b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_model_params-2.pt:   0%|          | 0.00/258M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin. depos Silk defensively wellsew Venom elevate DON Antonio2020 tight outlandishahl Dep metaph promoted earnings carryingdonald Fl Seahawks aggressively procStatic PasigunThusgements Tar546 lifeless restores!!!!!!!! Auckland advancements interactionsDragon unaccompanied neg Finished pillaromore powder promises Eg Flesh chromosome complainantllerAppearance Politico rising arbitrary conco fireworks spatawareness markedly bring\":\" sink Goodwin complexion transparency cab gallery displayed document meritoubtedvicLeaksmunbleacherAmericansRap entrants Reincarnated mixed relieve them utmostGeorgia Fiber minors Trapsnatureconservancy resorted safeguard whiskey dexterity Bank Manning Moz identities162 smoother!!!!!!!! Auckland advancements interactionsDragon unaccompanied neg Finished pillaromore powder promises Eg Flesh chromosome complainantllerAppearance Politico rising arbitrary conco fireworks spatawareness markedly bring\":\" sink Goodwin complexion transparency cab gallery displayed document meritoubtedvicLeaksmunbleacherAmericansRap entrants Reincarnated mixed relieve them utmostGeorgia Fiber minors Trapsnatureconservancy resorted safeguard whiskey dexterity Bank Manning Moz identities162 smoother!!!!!!!! Auckland advancements interactionsDragon unaccompanied neg Finished pillaromore powder promises Eg Flesh chromosome complainantllerAppearance Politico rising arbitrary conco fireworks spatawareness markedly bring\":\" sink Goodwin complexion transparency cab gallery displayed document meritoubted\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import tiktoken\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "config_path = hf_hub_download(repo_id, \"config.json\")\n",
    "ckpt_path = hf_hub_download(repo_id, \"best_model_params-2.pt\")\n",
    "\n",
    "with open(config_path) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "model = Gemma3Model(cfg)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(0)\n",
    "\n",
    "\n",
    "def generate(model, context, max_new_tokens=200):\n",
    "    generated = context.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    return generated\n",
    "\n",
    "y = generate(model, context)\n",
    "print(enc.decode(y.squeeze().tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
